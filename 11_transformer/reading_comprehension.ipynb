{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5634611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# TEAM MEMBERS:\n",
    "# Antonio Krizmanic - 2b193238-8e3c-11ec-986f-f39926f24a9c\n",
    "# Janek Putz - e31a3cae-8e6c-11ec-986f-f39926f24a9c\n",
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")  # Report only TF errors by default\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "\n",
    "from reading_comprehension_dataset import ReadingComprehensionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a179f01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--warmup_epochs'], dest='warmup_epochs', nargs=None, const=None, default=1, type=<class 'float'>, choices=None, help='Number of warmup epochs.', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# : Define reasonable defaults and optionally more parameters.\n",
    "# Also, you can set the number of the threads 0 to use all your CPU cores.\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--batch_size\", default=8, type=int, help=\"Batch size.\")\n",
    "parser.add_argument(\"--epochs\", default=1, type=int, help=\"Number of epochs.\")\n",
    "parser.add_argument(\"--seed\", default=42, type=int, help=\"Random seed.\")\n",
    "parser.add_argument(\"--threads\", default=1, type=int, help=\"Maximum number of threads to use.\")\n",
    "# additional parameter\n",
    "parser.add_argument(\"--decay\", default=\"None\", type=str, help=\"Learning decay rate type\")\n",
    "parser.add_argument(\"--learning_rate\", default=2e-5, type=float, help=\"Initial learning rate.\")\n",
    "parser.add_argument(\"--learning_rate_final\", default=1e-5, type=float, help=\"Final learning rate.\")\n",
    "parser.add_argument(\"--dropout\", default=0, type=float, help=\"Dropout\")\n",
    "parser.add_argument(\"--label_smoothing\", default=0.1, type=float, help=\"Label smoothing.\")\n",
    "parser.add_argument(\"--warmup_epochs\", default=1, type=float, help=\"Number of warmup epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ce385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWarmup(tf.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, warmup_steps, following_schedule):\n",
    "        self._warmup_steps = warmup_steps\n",
    "        self._warmup = tf.optimizers.schedules.PolynomialDecay(0., warmup_steps, following_schedule(0))\n",
    "        self._following = following_schedule\n",
    "\n",
    "    def __call__(self, step):\n",
    "        return tf.cond(step < self._warmup_steps,\n",
    "                       lambda: self._warmup(step),\n",
    "                       lambda: self._following(step - self._warmup_steps))\n",
    "\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, args, robeczech, train):\n",
    "\n",
    "        # A) REGULARIZATION\n",
    "        decay_steps = len(train) * args.epochs\n",
    "        if not args.decay or args.decay in [\"None\", \"none\"]:\n",
    "            # constant rate wrapped in callable schedule for warmup steps later\n",
    "            learning_rate = tf.keras.optimizers.schedules.PolynomialDecay(decay_steps=decay_steps,\n",
    "                                                                          initial_learning_rate=args.learning_rate,\n",
    "                                                                          end_learning_rate=args.learning_rate,\n",
    "                                                                          power=1.0)\n",
    "        else:\n",
    "            if args.decay == 'linear':\n",
    "                learning_rate = tf.keras.optimizers.schedules.PolynomialDecay(decay_steps=decay_steps,\n",
    "                                                                              initial_learning_rate=args.learning_rate,\n",
    "                                                                              end_learning_rate=args.learning_rate_final,\n",
    "                                                                              power=1.0)\n",
    "            elif args.decay == 'exponential':\n",
    "                decay_rate = args.learning_rate_final / args.learning_rate\n",
    "                learning_rate = tf.optimizers.schedules.ExponentialDecay(decay_steps=decay_steps,\n",
    "                                                                         decay_rate=decay_rate,\n",
    "                                                                         initial_learning_rate=args.learning_rate)\n",
    "            elif args.decay == 'cosine':\n",
    "                learning_rate = tf.keras.optimizers.schedules.CosineDecay(decay_steps=decay_steps,\n",
    "                                                                          initial_learning_rate=args.learning_rate)\n",
    "            else:\n",
    "                raise NotImplementedError(\"Use only 'linear', 'exponential' or 'cosine' as LR scheduler\")\n",
    "\n",
    "        # create warmup\n",
    "        warmup_steps = int(len(train) * args.warmup_epochs)  # len(train) -> number of steps in one epoch\n",
    "        learning_rate = LinearWarmup(warmup_steps, following_schedule=learning_rate)\n",
    "\n",
    "        # B) ARCHITECTURE\n",
    "        inputs = {\n",
    "            \"input_ids\": tf.keras.layers.Input(shape=[None], dtype=tf.int32, ragged=True),\n",
    "            \"attention_mask\": tf.keras.layers.Input(shape=[None], dtype=tf.int32, ragged=True)\n",
    "        }\n",
    "\n",
    "        robeczech_output = robeczech(input_ids=inputs[\"input_ids\"].to_tensor(),\n",
    "                                     attention_mask=inputs[\"attention_mask\"].to_tensor()).last_hidden_state\n",
    "        robeczech_dropout = tf.keras.layers.Dropout(args.dropout)(robeczech_output)\n",
    "\n",
    "        # [:, :, 0] removes the 3rd dimension and pulls the value in the second dimension (alternative to reshape)\n",
    "        # alternative: [..., 0] -> ... is placeholder for arbitrary number of dimensions\n",
    "        answer_start_output = tf.keras.layers.Dense(1)(robeczech_dropout)[:, :, 0]\n",
    "        answer_start_output_softmax = tf.keras.layers.Softmax()(answer_start_output)\n",
    "        answer_end_output = tf.keras.layers.Dense(1)(robeczech_dropout)[:, :, 0]\n",
    "        answer_end_output_softmax = tf.keras.layers.Softmax()(answer_end_output)\n",
    "        outputs = {\"answer_start\": answer_start_output_softmax, \"answer_end\": answer_end_output_softmax}\n",
    "\n",
    "        super().__init__(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        # C) COMPILE\n",
    "        self.compile(optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n",
    "                     loss={\"answer_start\": tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                           \"answer_end\": tf.keras.losses.SparseCategoricalCrossentropy()},\n",
    "                     metrics={\n",
    "                         \"answer_start\": [tf.metrics.SparseCategoricalAccuracy(name=\"accuracy_start\")],\n",
    "                         \"answer_end\": [tf.metrics.SparseCategoricalAccuracy(name=\"accuracy_end\")]\n",
    "                     })\n",
    "        self.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be79e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([] if \"__file__\" not in globals() else None)\n",
    "\n",
    "# Fix random seeds and threads\n",
    "tf.keras.utils.set_random_seed(args.seed)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(args.threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(args.threads)\n",
    "\n",
    "# Create logdir name\n",
    "args.logdir = os.path.join(\"logs\", \"{}-{}-{}\".format(\n",
    "    os.path.basename(globals().get(\"__file__\", \"notebook\")),\n",
    "    datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\"),\n",
    "    \",\".join((\"{}={}\".format(re.sub(\"(.)[^_]*_?\", r\"\\1\", k), v) for k, v in sorted(vars(args).items())))\n",
    "))\n",
    "\n",
    "# Load the Electra Czech small lowercased\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"ufal/robeczech-base\")\n",
    "robeczech = transformers.TFAutoModel.from_pretrained(\"ufal/robeczech-base\")\n",
    "\n",
    "# Load the data.\n",
    "reading_comprehension_dataset = ReadingComprehensionDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1846a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create datasets\n",
      "test dataset: 4900\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(name) -> (tf.data.Dataset, List[transformers.BatchEncoding], List[str]):\n",
    "    # Note: to improve it, before adding triples to result, check if computed answer end token index is in the\n",
    "    # range of the paragraph/context in the tokenized pair of context and question (otherwise we would point in the\n",
    "    # question -> train with false examples)\n",
    "    dataset = getattr(reading_comprehension_dataset, name)\n",
    "\n",
    "    # 1) tokenize data\n",
    "    context_question_pair_tokens, context_question_pair_attention_masks, answer_starts, answer_ends = [], [], [], []\n",
    "    encoded_context_question_pairs, inference_contexts = [], []\n",
    "    for paragraph in dataset.paragraphs:  # iterate over all paragraphs\n",
    "        for qa_pair in paragraph[\"qas\"]:  # iterate over all questions in a paragraph\n",
    "            encoded_context_question_pair = tokenizer(paragraph[\"context\"], qa_pair[\"question\"], max_length=512,\n",
    "                                                      truncation=\"only_first\")\n",
    "            # if dataset is test, append only to context_question_pairs, answers are empty\n",
    "            if name == 'test':\n",
    "                context_question_pair_tokens.append(encoded_context_question_pair[\"input_ids\"])\n",
    "                context_question_pair_attention_masks.append(encoded_context_question_pair[\"attention_mask\"])\n",
    "                # record raw and tokenized contexts for inference\n",
    "                inference_contexts.append(paragraph[\"context\"])\n",
    "                encoded_context_question_pairs.append(encoded_context_question_pair)\n",
    "            else:\n",
    "                for answer in qa_pair[\"answers\"]:  # iterate over all answers of a question\n",
    "                    # compute answer end index\n",
    "                    answer_end_token_idx = encoded_context_question_pair.char_to_token(\n",
    "                        answer[\"start\"] + len(answer[\"text\"]) - 1)\n",
    "                    # only if computation was successful, append whole triple (fails in 22 cases for train)\n",
    "                    if answer_end_token_idx is not None:\n",
    "                        # A) record for every answer the same tokenized context/question\n",
    "                        context_question_pair_tokens.append(encoded_context_question_pair[\"input_ids\"])\n",
    "                        context_question_pair_attention_masks.append(encoded_context_question_pair[\"attention_mask\"])\n",
    "                        # B) record for every answer the respective token IDs of start and end words\n",
    "                        answer_starts.append(encoded_context_question_pair.char_to_token(answer[\"start\"]))\n",
    "                        answer_ends.append(answer_end_token_idx)\n",
    "                        \n",
    "    # convert lists to tensors\n",
    "    context_question_pair_tokens = tf.ragged.constant(context_question_pair_tokens)\n",
    "    context_question_pair_attention_masks = tf.ragged.constant(context_question_pair_attention_masks)\n",
    "    answer_starts = tf.constant(answer_starts)\n",
    "    answer_ends = tf.constant(answer_ends)\n",
    "\n",
    "    # 2) create tf.data.DataSet\n",
    "    if name != 'test':\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((context_question_pair_tokens,\n",
    "                                                      context_question_pair_attention_masks,\n",
    "                                                      answer_starts,\n",
    "                                                      answer_ends))\n",
    "        # combine answer_start/tokens and answer_end/attention_mask to one output/input dict per sample\n",
    "        def map(context_question_pair_tokens, context_question_pair_attention_masks, answer_start, answer_end):\n",
    "            return {\"input_ids\": context_question_pair_tokens,\n",
    "                    \"attention_mask\": context_question_pair_attention_masks},\\\n",
    "                   {\"answer_start\": answer_start, \"answer_end\": answer_end}\n",
    "        dataset = dataset.map(map)\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((context_question_pair_tokens,\n",
    "                                                      context_question_pair_attention_masks))\n",
    "        # combine tokens and attention_mask to one input dict per sample\n",
    "        def map(context_question_pair_tokens, context_question_pair_attention_masks):\n",
    "            return {\"input_ids\": context_question_pair_tokens,\n",
    "                    \"attention_mask\": context_question_pair_attention_masks}\n",
    "        dataset = dataset.map(map)\n",
    "\n",
    "    if name == \"train\":\n",
    "        dataset = dataset.shuffle(buffer_size=10000, seed=args.seed)\n",
    "\n",
    "    print(f\"{name} dataset: {len(dataset)}\")\n",
    "    dataset = dataset.apply(tf.data.experimental.dense_to_ragged_batch(args.batch_size))\n",
    "    return dataset, encoded_context_question_pairs, inference_contexts\n",
    "\n",
    "print(\"create datasets\")\n",
    "# train = create_dataset(\"train\")\n",
    "#dev, _, _ = create_dataset(\"dev\")\n",
    "#train = dev # TODO\n",
    "test, test_contexts_tokenized, test_contexts = create_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c8d7c62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input.to_tensor (InstanceMetho  (None, None)        0           ['input_1[0][0]']                \n",
      " d)                                                                                               \n",
      "                                                                                                  \n",
      " input.to_tensor_1 (InstanceMet  (None, None)        0           ['input_2[0][0]']                \n",
      " hod)                                                                                             \n",
      "                                                                                                  \n",
      " tf_roberta_model_1 (TFRobertaM  TFBaseModelOutputWi  125948160  ['input.to_tensor[0][0]',        \n",
      " odel)                          thPoolingAndCrossAt               'input.to_tensor_1[0][0]']      \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, None                                               \n",
      "                                , 768),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dropout_74 (Dropout)           (None, None, 768)    0           ['tf_roberta_model_1[0][0]']     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, None, 1)      769         ['dropout_74[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 1)      769         ['dropout_74[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, None)        0           ['dense_1[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, None)        0           ['dense[0][0]']                  \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " softmax_1 (Softmax)            (None, None)         0           ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " softmax (Softmax)              (None, None)         0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 125,949,698\n",
      "Trainable params: 125,949,698\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# : Create the model and train it\n",
    "model = Model(args, robeczech, train)\n",
    "\n",
    "# model.fit(\n",
    "    #train.take(1), batch_size=args.batch_size, epochs=args.epochs, validation_data=dev.take(1),\n",
    "    #callbacks=[tf.keras.callbacks.TensorBoard(args.logdir, histogram_freq=1, update_freq=100, profile_batch=0),\n",
    "    #           tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=100,\n",
    "    #                                            verbose=0, mode=\"max\", baseline=None, restore_best_weights=True)]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba894629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "process batch\n",
      "105 132\n",
      "105 136\n",
      "105 110\n",
      "105 135\n",
      "105 110\n",
      "150 212\n",
      "150 212\n",
      "150 212\n",
      "\n",
      "process batch\n",
      "150 212\n",
      "76 76\n",
      "121 139\n",
      "121 132\n",
      "133 194\n",
      "transformers.tokenization_utils_base.CharSpan() argument after * must be an iterable, not NoneType\n",
      "14 99\n",
      "14 164\n",
      "14 85\n",
      "\n",
      "process batch\n",
      "14 99\n",
      "75 83\n",
      "5 192\n",
      "5 186\n",
      "192 201\n",
      "transformers.tokenization_utils_base.CharSpan() argument after * must be an iterable, not NoneType\n",
      "193 195\n",
      "5 89\n",
      "23 218\n",
      "\n",
      "process batch\n",
      "23 218\n",
      "23 218\n",
      "23 218\n",
      "2 78\n",
      "2 78\n",
      "2 78\n",
      "209 218\n",
      "209 226\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(args.logdir, exist_ok=True)\n",
    "with open(os.path.join(args.logdir, \"reading_comprehension.txt\"), \"w\", encoding=\"utf-8\") as predictions_file:\n",
    "    # : Predict the answers as strings, one per line.\n",
    "    n = 0\n",
    "    for test_batch in test.take(4):\n",
    "        print(\"\\nprocess batch\")\n",
    "        predictions = model.predict(test_batch)\n",
    "        predictions_starts = predictions[\"answer_start\"]\n",
    "        predictions_ends = predictions[\"answer_end\"]\n",
    "        \n",
    "        for batch_sample_i in range(len(predictions_ends)):\n",
    "            start_token_idx = tf.argmax(predictions_starts[batch_sample_i]).numpy()\n",
    "            # [start_token_idx:] is necessary to not retrieve smaller indices for end than for start token\n",
    "            end_token_idx = start_token_idx + tf.argmax(predictions_ends[batch_sample_i][start_token_idx:]).numpy()\n",
    "            # print(start_token_idx, end_token_idx)\n",
    "            try:\n",
    "                answer = test_contexts[n][test_contexts_tokenized[n].token_to_chars(start_token_idx).start:\n",
    "                                          test_contexts_tokenized[n].token_to_chars(end_token_idx - 1).end]\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "                answer = \"failed\"\n",
    "            n+=1\n",
    "            print(answer, file=predictions_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
