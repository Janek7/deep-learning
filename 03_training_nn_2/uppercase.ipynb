{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a072544",
   "metadata": {},
   "source": [
    "# Uppercase Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83752b6",
   "metadata": {},
   "source": [
    "## Contributors:\n",
    "\n",
    "<b>Antonio Krizmanic</b> - 2b193238-8e3c-11ec-986f-f39926f24a9c <br>\n",
    "<b>Janek Putz</b> - e31a3cae-8e6c-11ec-986f-f39926f24a9c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8edb4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\") # Report only TF errors by default\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from uppercase_data import UppercaseData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4434e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set reasonable values for the hyperparameters, notably\n",
    "# for `alphabet_size` and `window` and others.\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--alphabet_size\", default=1000, type=int, help=\"If nonzero, limit alphabet to this many most frequent chars.\")\n",
    "parser.add_argument(\"--batch_size\", default=256, type=int, help=\"Batch size.\")\n",
    "parser.add_argument(\"--epochs\", default=4, type=int, help=\"Number of epochs.\")\n",
    "parser.add_argument(\"--seed\", default=42, type=int, help=\"Random seed.\")\n",
    "parser.add_argument(\"--threads\", default=2, type=int, help=\"Maximum number of threads to use.\")\n",
    "parser.add_argument(\"--window\", default=13, type=int, help=\"Window size to use.\")\n",
    "# additional\n",
    "parser.add_argument(\"--dropout\", default=0.38, type=float, help=\"Dropout rate.\")\n",
    "parser.add_argument(\"--save_model\", default=True, type=bool, help=\"Flag if model should be saved.\")\n",
    "parser.add_argument(\"--model\", default=\"uppercase_model.h5\", type=str, help=\"Output model path.\")\n",
    "parser.add_argument(\"--hidden_layers\", default=3, nargs=\"*\", type=int, help=\"Hidden layer sizes.\")\n",
    "parser.add_argument(\"--l2\", default=0.0, type=float, help=\"L2 regularization.\")\n",
    "\n",
    "#Part of these is not used in the models themselve, but after 32+ hours of my laptop fih≈æghting\n",
    "#the models, I don't want to test my luck by removing some of them :(\n",
    "\n",
    "args = parser.parse_args([] if \"__file__\" not in globals() else None)\n",
    "   \n",
    "# Fix random seeds and threads\n",
    "tf.keras.utils.set_random_seed(args.seed)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(args.threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(args.threads)\n",
    "\n",
    "# Create logdir name\n",
    "args.logdir = os.path.join(\"logs-u\", \"{}-{}-{}\".format(\n",
    "    os.path.basename(globals().get(\"__file__\", \"notebook\")),\n",
    "    datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\"),\n",
    "    \",\".join((\"{}={}\".format(re.sub(\"(.)[^_]*_?\", r\"\\1\", key), value) for key, value in sorted(vars(args).items())))\n",
    "))[:99] # limit is necessary because of limited windows path length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc98c9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset uppercase_data.zip...\n"
     ]
    }
   ],
   "source": [
    "uppercase_data = UppercaseData(args.window, args.alphabet_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e9d4bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = tf.keras.regularizers.L2(l2 = 0.0)\n",
    "schedule = tf.keras.optimizers.schedules.ExponentialDecay(0.01,decay_steps=(uppercase_data.train.size/args.batch_size)*args.epochs,decay_rate=0.001/0.01,staircase=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417e212",
   "metadata": {},
   "source": [
    "## Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e4ce503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda (Lambda)             (None, 27, 579)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 15633)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               4002304   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,134,145\n",
      "Trainable params: 4,134,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/4\n",
      "23875/23875 - 1198s - loss: 0.0498 - binary_accuracy: 0.9820 - val_loss: 0.0407 - val_binary_accuracy: 0.9851 - 1198s/epoch - 50ms/step\n",
      "Epoch 2/4\n",
      "23875/23875 - 1138s - loss: 0.0350 - binary_accuracy: 0.9880 - val_loss: 0.0386 - val_binary_accuracy: 0.9867 - 1138s/epoch - 48ms/step\n",
      "Epoch 3/4\n",
      "23875/23875 - 1144s - loss: 0.0274 - binary_accuracy: 0.9909 - val_loss: 0.0376 - val_binary_accuracy: 0.9869 - 1144s/epoch - 48ms/step\n",
      "Epoch 4/4\n",
      "23875/23875 - 1131s - loss: 0.0212 - binary_accuracy: 0.9932 - val_loss: 0.0386 - val_binary_accuracy: 0.9874 - 1131s/epoch - 47ms/step\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=[2 * args.window + 1], dtype=tf.int32))\n",
    "model.add(tf.keras.layers.Lambda(lambda x: tf.one_hot(x, len(uppercase_data.train.alphabet))))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "for j in range(3):\n",
    "    model.add(tf.keras.layers.Dense(256, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=tf.keras.optimizers.schedules.PolynomialDecay(0.01,(uppercase_data.train.size/args.batch_size)*args.epochs,0.001,power=1)),\n",
    "    loss=tf.losses.BinaryCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(args.logdir, histogram_freq=1, update_freq=100, profile_batch=0)\n",
    "tb_callback._close_writers = lambda: None\n",
    "\n",
    "history = model.fit(\n",
    "    uppercase_data.train.data[\"windows\"], uppercase_data.train.data[\"labels\"],\n",
    "    batch_size=args.batch_size,\n",
    "    epochs=args.epochs,\n",
    "    validation_data=(uppercase_data.dev.data[\"windows\"], uppercase_data.dev.data[\"labels\"]),\n",
    "    callbacks=[tb_callback],\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f63450eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('uppercase_model.h5', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9828b1",
   "metadata": {},
   "source": [
    "## Trying other models\n",
    "The first model is the one used in shaping the solution .txt file, this one is just to try other options with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential()\n",
    "model2.add(tf.keras.layers.Input(shape=[2 * args.window + 1], dtype=tf.int32))\n",
    "model2.add(tf.keras.layers.Lambda(lambda x: tf.one_hot(x, len(uppercase_data.train.alphabet))))\n",
    "model2.add(tf.keras.layers.Flatten())\n",
    "for j in range(3):\n",
    "    model2.add(tf.keras.layers.Dense(512, activation=tf.nn.relu))\n",
    "    model2.add(tf.keras.layers.Dropout(rate=0.38))\n",
    "model2.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "\n",
    "\n",
    "model2.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate = tf.keras.optimizers.schedules.PolynomialDecay(0.01,(uppercase_data.train.size/1024)*10,0.001,power=1)),\n",
    "    # loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "    loss=tf.losses.BinaryCrossentropy(),\n",
    "    # metrics=[tf.metrics.SparseCategoricalAccuracy(\"accuracy\")],\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    ")\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(args.logdir, histogram_freq=1, update_freq=100, profile_batch=0)\n",
    "tb_callback._close_writers = lambda: None # A hack allowing to keep the writers open.\n",
    "\n",
    "history = model2.fit(\n",
    "    uppercase_data.train.data[\"windows\"], uppercase_data.train.data[\"labels\"],\n",
    "    batch_size=516,\n",
    "    epochs=8,\n",
    "    validation_data=(uppercase_data.dev.data[\"windows\"], uppercase_data.dev.data[\"labels\"]),\n",
    "    callbacks=[tb_callback],\n",
    "    verbose = 2\n",
    ")\n",
    "\n",
    "if args.save_model:\n",
    "    model2.save('uppercase_model2.h5', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06432d72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc641c33",
   "metadata": {},
   "source": [
    "## The creation of .txt solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7809b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('uppercase_model2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca03e7f",
   "metadata": {},
   "source": [
    "## Apply Model to Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "059bad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_preds_raw = model.predict(uppercase_data.test.data['windows'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "738cf94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure why I did this manually #\n",
    "for i in range(len(window_preds_raw)):\n",
    "    if window_preds_raw[i] < 0.3:\n",
    "        window_preds_raw[i] = 0\n",
    "    else:\n",
    "        window_preds_raw[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcd8a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"uppercase_test.txt\", \"w\", encoding=\"utf-8\") as predictions_file:\n",
    "    text = uppercase_data.test.text\n",
    "    for i, label in enumerate(window_preds_raw):\n",
    "        if label == 1:\n",
    "            text = text[:i] + text[i].upper() + text[i+1:]\n",
    "    predictions_file.write(text)     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
