{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a072544",
   "metadata": {},
   "source": [
    "# Uppercase Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01d53f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEAM MEMBERS:\n",
    "# Antonio Krizmanic - 2b193238-8e3c-11ec-986f-f39926f24a9c\n",
    "# Janek Putz - e31a3cae-8e6c-11ec-986f-f39926f24a9c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8edb4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\") # Report only TF errors by default\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from uppercase_data import UppercaseData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4434e2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(alphabet_size=100, batch_size=64, epochs=1, seed=42, threads=1, window=3, layers=2, h_size=400, dropout=0.5, save_model=True, model='uppercase_model.h5')\n"
     ]
    }
   ],
   "source": [
    "# TODO: Set reasonable values for the hyperparameters, notably\n",
    "# for `alphabet_size` and `window` and others.\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--alphabet_size\", default=100, type=int, help=\"If nonzero, limit alphabet to this many most frequent chars.\")\n",
    "parser.add_argument(\"--batch_size\", default=64, type=int, help=\"Batch size.\")\n",
    "parser.add_argument(\"--epochs\", default=1, type=int, help=\"Number of epochs.\")\n",
    "parser.add_argument(\"--seed\", default=42, type=int, help=\"Random seed.\")\n",
    "parser.add_argument(\"--threads\", default=1, type=int, help=\"Maximum number of threads to use.\")\n",
    "parser.add_argument(\"--window\", default=3, type=int, help=\"Window size to use.\")\n",
    "# additional\n",
    "parser.add_argument(\"--layers\", default=2, type=float, help=\"Number of hidden layers\")\n",
    "parser.add_argument(\"--h_size\", default=400, type=float, help=\"Size of hidden layers\")\n",
    "parser.add_argument(\"--dropout\", default=0.5, type=float, help=\"Dropout rate.\")\n",
    "parser.add_argument(\"--save_model\", default=True, type=bool, help=\"Flag if model should be saved.\")\n",
    "parser.add_argument(\"--model\", default=\"uppercase_model.h5\", type=str, help=\"Output model path.\")\n",
    "\n",
    "args = parser.parse_args([] if \"__file__\" not in globals() else None)\n",
    "print(args)\n",
    "\n",
    "# Fix random seeds and threads\n",
    "tf.keras.utils.set_random_seed(args.seed)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(args.threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(args.threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc98c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "uppercase_data = UppercaseData(args.window, args.alphabet_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cf13d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "['<pad>', '<unk>', ' ', 'o', 'e', 'a', 'n', 's', 't', 'i', 'l', 'r', 'v', 'k', 'd', 'm', 'u', 'p', 'c', 'í', 'h', 'z', 'á', 'j', 'b', 'y', '.', 'ě', 'é', ',', '\\n', 'ř', 'ý', 'č', 'ž', 'š', '1', 'g', 'f', 'ů', '0', '9', '(', ')', '2', '8', '5', '3', '4', '6', 'ú', 'w', '7', '-', 'x', 'ň', '–', 'ó', '„', '“', 'ť', ':', '\"', '/', 'ď', ';', 'q', \"'\", '%', 'ö', '*', 'ü', 'ä', '°', '+', 'а', 'с', 'о', 'ł', '&', '=', 'н', 'и', '!', '²', 'е', '’', 'ë', 'ć', 'р', '?', 'к', 'т', '…', 'è', 'в', 'ľ', '´', 'ç', '†']\n"
     ]
    }
   ],
   "source": [
    "print(len(uppercase_data.train.alphabet))\n",
    "print(uppercase_data.train.alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff039da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = uppercase_data.train.data\n",
    "for i, (window, label) in enumerate(zip(train_data['windows'], train_data['labels'])):\n",
    "    if i < 20:\n",
    "        print(i, window, [uppercase_data.train.alphabet[i] for i in window], label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417e212",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e4ce503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda (Lambda)             (None, 7, 100)            0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 700)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 700)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 400)               280400    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 400)               160400    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 401       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 441,201\n",
      "Trainable params: 441,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "95500/95500 [==============================] - 1727s 18ms/step - loss: 0.1156 - binary_accuracy: 0.9650 - val_loss: 0.0952 - val_binary_accuracy: 0.9659\n"
     ]
    }
   ],
   "source": [
    "# Create logdir name\n",
    "args.logdir = os.path.join(\"logs-u\", \"{}-{}-{}\".format(\n",
    "    os.path.basename(globals().get(\"__file__\", \"notebook\")),\n",
    "    datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\"),\n",
    "    \",\".join((\"{}={}\".format(re.sub(\"(.)[^_]*_?\", r\"\\1\", key), value) for key, value in sorted(vars(args).items())))\n",
    "))\n",
    "\n",
    "\n",
    "# TODO: Implement a suitable model, optionally including regularization, select\n",
    "# good hyperparameters and train the model.\n",
    "#\n",
    "# The inputs are _windows_ of fixed size (`args.window` characters on left,\n",
    "# the character in question, and `args.window` characters on right), where\n",
    "# each character is represented by a `tf.int32` index. To suitably represent\n",
    "# the characters, you can:\n",
    "# - Convert the character indices into _one-hot encoding_. There is no\n",
    "#   explicit Keras layer, but you can\n",
    "#   - use a Lambda layer which can encompass any function:\n",
    "#       tf.keras.Sequential([\n",
    "#         tf.keras.layers.Input(shape=[2 * args.window + 1], dtype=tf.int32),\n",
    "#         tf.keras.layers.Lambda(lambda x: tf.one_hot(x, len(uppercase_data.train.alphabet))),\n",
    "#   - or use Functional API and then any TF function can be used\n",
    "#     as a Keras layer:\n",
    "#       inputs = tf.keras.layers.Input(shape=[2 * args.window + 1], dtype=tf.int32)\n",
    "#       encoded = tf.one_hot(inputs, len(uppercase_data.train.alphabet))\n",
    "#   You can then flatten the one-hot encoded windows and follow with a dense layer.\n",
    "# - Alternatively, you can use `tf.keras.layers.Embedding` (which is an efficient\n",
    "#   implementation of one-hot encoding followed by a Dense layer) and flatten afterwards.\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=[2 * args.window + 1], dtype=tf.int32))\n",
    "model.add(tf.keras.layers.Lambda(lambda x: tf.one_hot(x, len(uppercase_data.train.alphabet))))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dropout(rate=args.dropout))\n",
    "for i in range(0, args.layers):\n",
    "    model.add(tf.keras.layers.Dense(args.h_size, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dropout(rate=args.dropout))\n",
    "# variant 1\n",
    "# model.add(tf.keras.layers.Dense(2, activation=tf.nn.softmax))\n",
    "# variant 2: binary classification\n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "          \n",
    "# TODO: try with one large dense layer (400)\n",
    "    \n",
    "    \n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    # loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "    loss=tf.losses.BinaryCrossentropy(),\n",
    "    # metrics=[tf.metrics.SparseCategoricalAccuracy(\"accuracy\")],\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# tensorboard --logdir \"C:\\\\Users\\\\janek\\\\Development\\\\Git\\\\Prag\\\\deep-learning-lecture\\\\03_training_nn_2\\\\logs-u\"\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(args.logdir, histogram_freq=1, update_freq=100, profile_batch=0)\n",
    "tb_callback._close_writers = lambda: None # A hack allowing to keep the writers open.\n",
    "\n",
    "history = model.fit(\n",
    "    uppercase_data.train.data[\"windows\"], uppercase_data.train.data[\"labels\"],\n",
    "    batch_size=args.batch_size,\n",
    "    epochs=args.epochs,\n",
    "    validation_data=(uppercase_data.dev.data[\"windows\"], uppercase_data.dev.data[\"labels\"]),\n",
    "    callbacks=[tb_callback]\n",
    ")\n",
    "\n",
    "if args.save_model:\n",
    "    model.save(os.path.join(args.logdir, args.model), include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06432d72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.1172260046005249], 'binary_accuracy': [0.9646154642105103], 'val_loss': [0.09283903986215591], 'val_binary_accuracy': [0.965552568435669]}\n"
     ]
    }
   ],
   "source": [
    "# One 64 dense layer\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6262462d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.1370404213666916], 'binary_accuracy': [0.9584030508995056], 'val_loss': [0.15306855738162994], 'val_binary_accuracy': [0.9572327733039856]}\n"
     ]
    }
   ],
   "source": [
    "# One 6 x 64 dense layer\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c7701bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.12071765959262848], 'binary_accuracy': [0.9639292359352112], 'val_loss': [0.0895470455288887], 'val_binary_accuracy': [0.965847373008728]}\n"
     ]
    }
   ],
   "source": [
    "# One 3 x 64 dense layer  # Same with 5 epochs\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dcc6f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.11876511573791504], 'binary_accuracy': [0.9643425345420837], 'val_loss': [0.09459041804075241], 'val_binary_accuracy': [0.9656572937965393]}\n"
     ]
    }
   ],
   "source": [
    "# One 3 x 128 dense layer\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca03e7f",
   "metadata": {},
   "source": [
    "## Apply Model to Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aab10ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11373/11373 [==============================] - 34s 3ms/step - loss: 0.1397 - binary_accuracy: 0.9913\n",
      "[0.13965097069740295, 0.9912923574447632]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(uppercase_data.test.data['windows'], uppercase_data.test.data['labels'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "059bad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_preds_raw = model.predict(uppercase_data.test.data['windows'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302deb71",
   "metadata": {},
   "source": [
    "### New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61e8c09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363932\n",
      "363932\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(args.logdir, \"uppercase_test.txt\"), \"w\", encoding=\"utf-8\") as predictions_file:\n",
    "    text = uppercase_data.test.text\n",
    "    print(len(text))\n",
    "    for i, label in enumerate(window_preds_raw):\n",
    "        if label == 1:\n",
    "            text = text[:i] + text[i].upper() + text[i+1:]\n",
    "    print(len(text))\n",
    "    predictions_file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8cd51",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcd8a4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[360608   3324]\n",
      "6\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "# : Generate correctly capitalized test set.\n",
    "# Use `uppercase_data.test.text` as input, capitalize suitable characters,\n",
    "# and write the result to predictions_file (which is\n",
    "# `uppercase_test.txt` in the `args.logdir` directory).\n",
    "os.makedirs(args.logdir, exist_ok=True)\n",
    "with open(os.path.join(args.logdir, \"uppercase_test.txt\"), \"w\", encoding=\"utf-8\") as predictions_file:\n",
    "    window_preds = np.reshape(window_preds_raw, -1)\n",
    "    \n",
    "    # check if model predicts binary or two class and transform preds to 0 and 1\n",
    "    if window_preds_raw[0].shape[0] == 1:\n",
    "        window_preds = np.where(window_preds > 0.5, 1, 0)\n",
    "    elif window_preds_raw[0].shape[0] == 2:\n",
    "        window_preds = np.where(window_preds[1] > 0.5, 1, 0)\n",
    "    print(np.bincount(window_preds))\n",
    "    \n",
    "    # process windows\n",
    "    final_letters = []\n",
    "    uppercase_indexes = []\n",
    "    for i, (window, label) in enumerate(zip(uppercase_data.test.data['windows'], window_preds)):\n",
    "        # extend final list of letters with last letter from new window\n",
    "        if i == 0:\n",
    "            final_letters = [uppercase_data.train.alphabet[i] for i in window]\n",
    "        else:\n",
    "            final_letters.append(uppercase_data.train.alphabet[window[-1]])\n",
    "        # if label is 1, add middle letter to list of uppercase letters\n",
    "        if label == 1:\n",
    "            uppercase_indexes.append(i + (args.window))\n",
    "    # replace uppercase letters\n",
    "    final_letters = [letter if i not in uppercase_indexes else letter.upper() for i, letter in enumerate(final_letters) ]\n",
    "    \n",
    "    # concat text\n",
    "    text = ''.join(final_letters)\n",
    "    print(text.count(\"<pad>\"))\n",
    "    print(text.count(\"<unk>\"))\n",
    "    text = text.replace('<pad>', ' ')\n",
    "    text = text.replace('<unk>', ' ')\n",
    "    predictions_file.write(text)\n",
    "    \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
