{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9a4476f",
   "metadata": {},
   "source": [
    "# SVHN Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bf05b0",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cce8d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# TEAM MEMBERS:\n",
    "# Antonio Krizmanic - 2b193238-8e3c-11ec-986f-f39926f24a9c\n",
    "# Janek Putz - e31a3cae-8e6c-11ec-986f-f39926f24a9c\n",
    "import argparse\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")  # Report only TF errors by default\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "# logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "logger = logging.getLogger('SVHN')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import bboxes_utils\n",
    "import efficient_net\n",
    "from svhn_dataset import SVHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52e00ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--batch_size\", default=50, type=int, help=\"Batch size.\")\n",
    "parser.add_argument(\"--epochs\", default=2, type=int, help=\"Number of epochs.\")\n",
    "parser.add_argument(\"--seed\", default=42, type=int, help=\"Random seed.\")\n",
    "parser.add_argument(\"--threads\", default=1, type=int, help=\"Maximum number of threads to use.\")\n",
    "parser.add_argument(\"--logging_level\", default=\"info\", type=str, help=\"Logging level\")\n",
    "\n",
    "parser.add_argument(\"--fine_tuning\", default=False, type=bool, help=\"Optionally fine tune the efficient net core.\")\n",
    "parser.add_argument(\"--level\", default=4, type=bool, help=\"Level of pyramid of efficient net to use as base.\")\n",
    "parser.add_argument(\"--image_size\", default=224, type=int, help=\"Width and height to resize image to uniform size.\")\n",
    "parser.add_argument(\"--conv_filters\", default=256, type=int, help=\"Number of filters in conv layers in heads.\")\n",
    "parser.add_argument(\"--iou_threshold\", default=0.5, type=float, help=\"Threshold to assign anchors to gold bboxes.\")\n",
    "parser.add_argument(\"--iou_prediction\", default=0.5, type=float, help=\"Threshold for non max suppresion.\")\n",
    "parser.add_argument(\"--score_threshold\", default=0.2, type=float, help=\"Score threshold for non max suppresion.\")\n",
    "\n",
    "parser.add_argument(\"--batch_norm\", default=True, type=bool, help=\"Batch normalization of conv. layers.\")\n",
    "parser.add_argument(\"--dropout\", default=0.5, type=float, help=\"Dropout rate after efficient net layer.\")\n",
    "parser.add_argument(\"--l2\", default=0.00, type=float, help=\"L2 regularization.\")\n",
    "parser.add_argument(\"--decay\", default=\"cosine\", type=str, help=\"Learning decay rate type\")\n",
    "parser.add_argument(\"--learning_rate\", default=0.001, type=float, help=\"Initial learning rate.\")\n",
    "parser.add_argument(\"--learning_rate_final\", default=0.0001, type=float, help=\"Final learning rate.\")\n",
    "\n",
    "# todo: try batch=1 without resizing\n",
    "\n",
    "\n",
    "args = parser.parse_args([] if \"__file__\" not in globals() else None)\n",
    "\n",
    "# Create logdir name\n",
    "args.logdir = os.path.join(\"logs\", \"{}-{}-{}\".format(\n",
    "    os.path.basename(globals().get(\"__file__\", \"notebook\")),\n",
    "    datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\"),\n",
    "    \",\".join((\"{}={}\".format(re.sub(\"(.)[^_]*_?\", r\"\\1\", k), v) for k, v in sorted(vars(args).items())))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d031ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seeds and threads\n",
    "np.random.seed(args.seed)\n",
    "tf.random.set_seed(args.seed)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(args.threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(args.threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d57664",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53925b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchors: (196, 4)\n"
     ]
    }
   ],
   "source": [
    "# create anchors\n",
    "def anchors_new():\n",
    "    anchors = np.array([[-1, -1, -1, -1]])\n",
    "    square_anchor_size = 2**args.level * 4  # for vertical boxes use 2^l*4 for height and 2^l*2 for width\n",
    "    for row in range(2**args.level, 2**args.level * 14 + 1, 2**args.level):\n",
    "        for col in range(2**args.level, 2**args.level * 14 + 1, 2**args.level):\n",
    "            anchors = np.append(anchors, [[row - square_anchor_size/2,\n",
    "                                           col - square_anchor_size/2, \n",
    "                                           row + square_anchor_size/2, \n",
    "                                           col + square_anchor_size/2]], axis=0)\n",
    "    anchors = np.delete(anchors, 0, 0)\n",
    "    print(\"anchors:\", anchors.shape)\n",
    "    # print(anchors)\n",
    "    return anchors\n",
    "\n",
    "anchors = anchors_new()\n",
    "\n",
    "\n",
    "# Load the data\n",
    "svhn = SVHN()\n",
    "\n",
    "def create_dataset(dataset: tf.data.Dataset, training: bool) -> tf.data.Dataset:\n",
    "\n",
    "    def prepare_data(example):\n",
    "        example[\"classes\"] = tf.cast(example[\"classes\"], dtype=tf.int32)\n",
    "        example[\"bboxes\"] = example[\"bboxes\"] / tf.cast(tf.shape(example[\"image\"])[0], tf.float32)\n",
    "        resized_image = tf.image.resize(example[\"image\"], [args.image_size, args.image_size])\n",
    "        \n",
    "        anchor_classes, anchor_bboxes = tf.numpy_function(\n",
    "            bboxes_utils.bboxes_training, # name\n",
    "            [anchors, example[\"classes\"], example[\"bboxes\"], args.iou_threshold], # param values\n",
    "            (tf.int32, tf.float32) # return types\n",
    "        )\n",
    "        anchor_classes_one_hot = tf.one_hot(anchor_classes - 1, SVHN.LABELS)\n",
    "        \n",
    "        output = {\n",
    "            \"classes\": tf.ensure_shape(anchor_classes_one_hot, [len(anchors), SVHN.LABELS]),\n",
    "            \"bboxes\": tf.ensure_shape(anchor_bboxes, [len(anchors), 4])\n",
    "        }\n",
    "        \n",
    "        sample_weights = {\n",
    "            \"classes\": 1,\n",
    "            \"bboxes\": tf.cast(anchor_classes > 0, tf.int32)\n",
    "        }\n",
    "        return resized_image, output, sample_weights\n",
    "    \n",
    "    if training:\n",
    "        dataset = dataset.map(prepare_data)\n",
    "        dataset = dataset.shuffle(buffer_size=10000, seed=args.seed)\n",
    "    else:\n",
    "        dataset = dataset.map(lambda example: (tf.image.resize(example[\"image\"], [args.image_size, args.image_size]), tf.shape(example[\"image\"])))\n",
    "    dataset = dataset.batch(args.batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b30cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = create_dataset(svhn.train, True)\n",
    "dev = create_dataset(svhn.dev, False)\n",
    "test = create_dataset(svhn.test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7552343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in dev:\n",
    "    print(b)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d9f63",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d2bd3e",
   "metadata": {},
   "source": [
    "### Load EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83e7ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dynamic_input_shape in case of batching with size 1 and different sizes\n",
    "efficientnet_b0 = efficient_net.pretrained_efficientnet_b0(include_top=False, dynamic_input_shape=False)\n",
    "efficientnet_b0.trainable = args.fine_tuning\n",
    "#for o in efficientnet_b0.outputs:\n",
    "#    print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0aead0",
   "metadata": {},
   "source": [
    "### Parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0281e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.l2:\n",
    "    regularizer = tf.keras.regularizers.L2(args.l2)\n",
    "else:\n",
    "    regularizer = None\n",
    "\n",
    "def bn_relu(input):\n",
    "    if args.batch_norm:\n",
    "        return tf.keras.layers.ReLU()(tf.keras.layers.BatchNormalization()(input))\n",
    "    else:\n",
    "        return tf.keras.layers.ReLU()(input)\n",
    "    \n",
    "if not args.decay or args.decay in [\"None\", \"none\"]:\n",
    "    learning_rate = args.learning_rate\n",
    "else:\n",
    "    decay_steps = (len(train) / args.batch_size) * args.epochs\n",
    "    if args.decay == 'linear':\n",
    "        learning_rate = tf.keras.optimizers.schedules.PolynomialDecay(decay_steps=decay_steps,\n",
    "                                                                      initial_learning_rate=args.learning_rate,\n",
    "                                                                      end_learning_rate=args.learning_rate_final,\n",
    "                                                                      power=1.0)\n",
    "    elif args.decay == 'exponential':\n",
    "        decay_rate = args.learning_rate_final / args.learning_rate\n",
    "        learning_rate = tf.optimizers.schedules.ExponentialDecay(decay_steps=decay_steps,\n",
    "                                                                 decay_rate=decay_rate,\n",
    "                                                                 initial_learning_rate=args.learning_rate)\n",
    "    elif args.decay == 'cosine':\n",
    "        learning_rate = tf.keras.optimizers.schedules.CosineDecay(decay_steps=decay_steps,\n",
    "                                                                  initial_learning_rate=args.learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Use only 'linear', 'exponential' or 'cosine' as LR scheduler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4cfbe7",
   "metadata": {},
   "source": [
    "### Compose Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbe52be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, args: argparse.Namespace) -> None:\n",
    "        \n",
    "        inputs = tf.keras.Input(shape=(args.image_size, args.image_size, 3))\n",
    "\n",
    "        pyramid_output = efficientnet_b0(inputs)[len(efficientnet_b0.outputs) - args.level]\n",
    "        eff_representation_size = int(args.image_size / 2**args.level)  # 14\n",
    "        pyramid_output = tf.keras.layers.Dropout(args.dropout)(pyramid_output)\n",
    "\n",
    "        # classification head (TODO: try more layers)\n",
    "        classes_conv1 = bn_relu(tf.keras.layers.Conv2D(args.conv_filters, 3, 1, \"same\", kernel_regularizer=regularizer)(pyramid_output))\n",
    "        classes_conv2 = bn_relu(tf.keras.layers.Conv2D(args.conv_filters, 3, 1, \"same\", kernel_regularizer=regularizer)(classes_conv1))\n",
    "        classes_conv3 = bn_relu(tf.keras.layers.Conv2D(args.conv_filters, 3, 1, \"same\", kernel_regularizer=regularizer)(classes_conv2))\n",
    "        classes_conv4 = tf.keras.layers.Conv2D(SVHN.LABELS, 3, 1, \"same\", activation=tf.nn.sigmoid, kernel_regularizer=regularizer)(classes_conv3)\n",
    "        classes_output_reshaped = tf.keras.layers.Reshape((eff_representation_size**2, SVHN.LABELS), name=\"classes\")(classes_conv4)\n",
    "\n",
    "        # bbox regression head (TODO: try more layers)\n",
    "        bbox_conv1 = bn_relu(tf.keras.layers.Conv2D(args.conv_filters, 3, 1, \"same\", kernel_regularizer=regularizer)(pyramid_output))\n",
    "        bbox_conv2 = bn_relu(tf.keras.layers.Conv2D(args.conv_filters, 3, 1, \"same\", kernel_regularizer=regularizer)(bbox_conv1))\n",
    "        bbox_conv3 = bn_relu(tf.keras.layers.Conv2D(args.conv_filters, 3, 1, \"same\", kernel_regularizer=regularizer)(bbox_conv2))\n",
    "        bbox_conv4 = tf.keras.layers.Conv2D(4, 3, 1, \"same\", kernel_regularizer=regularizer)(bbox_conv3)\n",
    "        bbox_output_reshaped = tf.keras.layers.Reshape((eff_representation_size**2, 4), name=\"bboxes\")(bbox_conv4)\n",
    "\n",
    "        outputs = {\n",
    "            \"classes\": classes_output_reshaped,\n",
    "            \"bboxes\": bbox_output_reshaped\n",
    "        }\n",
    "\n",
    "        super().__init__(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        self.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss={  # keys fit to output dict\n",
    "                \"classes\": tf.keras.losses.BinaryFocalCrossentropy(),\n",
    "                \"bboxes\": tf.keras.losses.Huber()\n",
    "            },\n",
    "            metrics={\n",
    "                \"classes\": [],\n",
    "                \"bboxes\": []  # mse for regression\n",
    "            }  # better call dataset.evaluate instead -> implement as callback (example: example_keras_models in 03)\n",
    "        )\n",
    "        \n",
    "    @staticmethod\n",
    "    def bboxes_from_fast_rcnn_batch(anchors, fast_rcnn_batch):\n",
    "        batch_elements = tf.unstack(fast_rcnn_batch)\n",
    "        processed = []\n",
    "        for element in batch_elements:\n",
    "            result = bboxes_utils.bboxes_from_fast_rcnn(anchors, element)\n",
    "            processed.append(result)\n",
    "        output = tf.stack(processed)\n",
    "        return output\n",
    "\n",
    "    # Override `predict_step` to perform non-max suppression and rescaling of bounding boxes\n",
    "    def predict_step(self, data):\n",
    "        # tf.print(\"enter predict step\")\n",
    "        images, sizes = data\n",
    "        \n",
    "        # predict\n",
    "        y_pred = self(images, training=False)\n",
    "        classes, bboxes = y_pred[\"classes\"], y_pred[\"bboxes\"]\n",
    "\n",
    "        # transform bboxes after NN back to normal representation\n",
    "        # tf.print(\"bboxes shape\", tf.shape(bboxes))\n",
    "        bboxes = tf.numpy_function(\n",
    "            self.bboxes_from_fast_rcnn_batch, # name\n",
    "            [anchors, bboxes], # param values\n",
    "            (tf.float32) # return types\n",
    "        )\n",
    "        # tf.print(\"bboxes shape after transform\", tf.shape(bboxes))\n",
    "        # tf.print(\"classes shape\", tf.shape(classes))\n",
    "        \n",
    "        # non max suppression\n",
    "        bboxes, scores, classes, valid_detections = tf.image.combined_non_max_suppression(\n",
    "            bboxes[:, :, tf.newaxis], classes, 5, 5, args.iou_prediction, score_threshold=args.score_threshold)\n",
    "        #tf.print(\"---------\")\n",
    "        #tf.print(\"valid_detections.shape\", tf.shape(valid_detections))\n",
    "        #tf.print(valid_detections)\n",
    "        #tf.print(tf.unique(valid_detections))\n",
    "        #tf.print(\"---------\")\n",
    "        #tf.print(\"bboxes.shape\", tf.shape(bboxes))\n",
    "        #tf.print(\"bboxes\", bboxes)\n",
    "        #tf.print(\"---------\")\n",
    "\n",
    "        # resize bboxes to original size\n",
    "        bboxes *= tf.cast(sizes[:, 0], tf.float32)[:, tf.newaxis, tf.newaxis]\n",
    "        \n",
    "        return classes, bboxes, valid_detections\n",
    "\n",
    "model = Model(args)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60b92a1",
   "metadata": {},
   "source": [
    "## Train & Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a4993f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions(model, data, filename):\n",
    "\n",
    "    # Generate test set annotations, but in `args.logdir` to allow parallel execution.\n",
    "    os.makedirs(args.logdir, exist_ok=True)\n",
    "    with open(os.path.join(args.logdir, filename), \"w\", encoding=\"utf-8\") as predictions_file:\n",
    "        # : Predict the digits and their bounding boxes on the test set.\n",
    "        # Assume that for a single test image we get\n",
    "        # - `predicted_classes`: a 1D array with the predicted digits,\n",
    "        # - `predicted_bboxes`: a [len(predicted_classes), 4] array with bboxes;\n",
    "        predictions = model.predict(data)\n",
    "        predicted_classes, predicted_bboxes, valid_detections = predictions\n",
    "\n",
    "        for test_image_idx in range(predicted_classes.shape[0]):\n",
    "            output = []\n",
    "            # limit outputs to valid outputs from non max suppression\n",
    "            for valid_idx in range(valid_detections[test_image_idx]):\n",
    "                label = int(predicted_classes[test_image_idx][valid_idx])\n",
    "                bbox = predicted_bboxes[test_image_idx][valid_idx]\n",
    "\n",
    "                output += [label] + list(bbox)\n",
    "            print(*output, file=predictions_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc33041",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint_path = os.path.join(args.logdir, \"svhn_competition.ckpt\")\n",
    "\n",
    "def evaluate_dev(epoch, logs):\n",
    "    filename = \"svhn_dev.txt\"\n",
    "    # create predictions in file\n",
    "    create_predictions(model, dev, filename)\n",
    "    # read file and evaluate it\n",
    "    with open(os.path.join(args.logdir, filename), \"r\", encoding=\"utf-8-sig\") as predictions_file:\n",
    "        accuracy = SVHN.evaluate_file(svhn.dev, predictions_file)\n",
    "        logs.update({\"val_accuracy\": accuracy})\n",
    "\n",
    "model.fit(\n",
    "    train.take(1), batch_size=args.batch_size, epochs=args.epochs,\n",
    "    callbacks=[tf.keras.callbacks.TensorBoard(args.logdir, histogram_freq=1, update_freq=100, profile_batch=0),\n",
    "               tf.keras.callbacks.LambdaCallback(on_epoch_end=evaluate_dev),\n",
    "               tf.keras.callbacks.ModelCheckpoint(filepath=best_checkpoint_path,\n",
    "                                                   save_weights_only=False, monitor='val_accuracy',\n",
    "                                                   mode='max', save_best_only=True)]\n",
    ")\n",
    "\n",
    "best_model = tf.keras.models.load_model(best_checkpoint_path)\n",
    "\n",
    "create_predictions(best_model, test, \"svhn_competition.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a717d02d",
   "metadata": {},
   "source": [
    "### Small tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40555214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs\\notebook-2022-04-07_105724-bn=True,bs=50,cf=256,d=None,d=0.5,e=2,ft=False,is=224,ip=0.5,it=0.5,l=0.0,lr=0.001,lrf=0.0001,l=4,ll=info,st=0.2,s=42,t=1\n"
     ]
    }
   ],
   "source": [
    "print(args.logdir)\n",
    "create_predictions(model, dev, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4dcbb476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4075, 'val_accuracy': 0.0}\n"
     ]
    }
   ],
   "source": [
    "filename = \"svhn_dev.txt\"\n",
    "# read file and evaluate it\n",
    "logs_test = {\"loss\": 0.4075}\n",
    "with open(os.path.join(args.logdir, filename), \"r\", encoding=\"utf-8-sig\") as predictions_file:\n",
    "    accuracy = SVHN.evaluate_file(svhn.dev, predictions_file)\n",
    "    logs_test.update({\"val_accuracy\": accuracy})\n",
    "\n",
    "print(logs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e63fc3",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84ce21e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 4)\n",
      "(2, 5)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "bboxes = np.array([[\n",
    "                            [1,0,0,1],\n",
    "                            [1,0,1,1],\n",
    "                            [0,0,0,0],\n",
    "                            [0,0,0,0],\n",
    "                            [0,0,0,0]\n",
    "                        ],\n",
    "                        [\n",
    "                            [1,1,0,0],\n",
    "                            [1,0,1,1],\n",
    "                            [0,1,1,0],\n",
    "                            [0,0,0,0],\n",
    "                            [0,0,0,0]\n",
    "                        ]\n",
    "])\n",
    "\n",
    "classes = np.array([[\n",
    "                            6,3,0,0,0\n",
    "                        ],\n",
    "                        [\n",
    "                            7,1,9,0,0\n",
    "                        ]\n",
    "])\n",
    "\n",
    "valid_detections = np.array([2,3])\n",
    "\n",
    "print(bboxes.shape)\n",
    "print(classes.shape)\n",
    "print(valid_detections.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d6ea4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: 0\n",
      "valid detections: 2\n",
      "6 1 0 0 1\n",
      "3 1 0 1 1\n",
      "------\n",
      "image: 1\n",
      "valid detections: 3\n",
      "7 1 1 0 0\n",
      "1 1 0 1 1\n",
      "9 0 1 1 0\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(args.logdir, \"svhn_competition.txt\"), \"w\", encoding=\"utf-8\") as predictions_file:\n",
    "\n",
    "    for test_image_idx in range(classes.shape[0]):\n",
    "        print(\"image:\", test_image_idx)\n",
    "        print(\"valid detections:\", valid_detections[test_image_idx])\n",
    "        outputs = []\n",
    "        for valid_idx in range(valid_detections[test_image_idx]):\n",
    "            label = classes[test_image_idx][valid_idx]\n",
    "            bbox = bboxes[test_image_idx][valid_idx]\n",
    "\n",
    "            print(*[label] + list(bbox))\n",
    "            outputs += [label] + list(bbox)\n",
    "        print(*outputs, file=predictions_file)\n",
    "        print(\"------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
